2.3 Universal User Representation Deep representation learning framework to obtain universal user representations is in a beginning stage. Ni et al. [18] perform multitask representation learning using an attention-based RNN architecture to capture in-depth representations of portal users. They assume that learning multiple tasks, e.g., predicting CTR, price preference, shop preference, etc., at once can produce better user representations. Yuan et al. [32] propose parameter-efficient transfer learning architecture named PeterRec to relieve the computational cost burden of fine-tuning. PeterRec covers five downstream tasks that include predicting user profiles like gender, age, and life status, as well as a cold-start recommendation task for browser recommender systems. Zhang et al. [33] train autoencoder-coupled Transformer networks that model retention, installation, and uninstallation collectively. They test the user embeddings in three downstream tasks for mobile app management scenarios. Gu et al. [9] propose a behavioral consistency loss to preserve the user’s longterm interest and an aggregation scheme for the benefit of model capacity. The proposed method is evaluated on user preference prediction and user profiling tasks. The pre-trained models of [9, 33] play the role of feature extractor like ours. Following the previous studies, we propose a framework to learn general-purpose user representation using self-supervised transfer learning in e-commerce platforms. Specifically, ShopperBERT pre-trains user embedding by optimizing nine pretext tasks constructed from user purchase history. This paper investigates the effectiveness of the pre-trained representation on heterogeneous e-commerce tasks including user profiling, targeting, and recommendation problems.
from 《One4all User Representation for Recommender Systems inE-commerce》
